%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[sigconf]{acmart}

\usepackage[english]{babel}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\usepackage{titlesec} % Allows customization of titles
\titleformat{\section}[block]{\large\scshape\leftalign}{\thesection.}{1em}{}
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{}

\usepackage{hyperref}

%----------------------------------------------------------------------------------------
%	TITLE & AUTHORS SECTION
%----------------------------------------------------------------------------------------

\begin{document}

\title{Recognizing emotions with technology and starting discussions}
\subtitle{Research in Emerging Technologies 2017-2018, final paper}
\date{January 2018}

\author{Randall Theuns}
\affiliation{%
    \institution{Amsterdam University of Applied Sciences}
    \department{Software Engineering}
    \streetaddress{Wibautstraat 2-4}
    \city{Amsterdam}
    \postcode{1091GM}
    \country{The Netherlands}
}
\email{randall.theuns@hva.nl}

\author{Max Beije}
\affiliation{%
    \institution{Amsterdam University of Applied Sciences}
    \department{Business IT \& Management}
    \streetaddress{Wibautstraat 2-4}
    \city{Amsterdam}
    \postcode{1091GM}
    \country{The Netherlands}
}
\email{max.beije@hva.nl}

\author{Frank Portengen}
\affiliation{%
    \institution{Amsterdam University of Applied Sciences}
    \department{Business IT \& Management}
    \streetaddress{Wibautstraat 2-4}
    \city{Amsterdam}
    \postcode{1091GM}
    \country{The Netherlands}
}
\email{frank.portengen@hva.nl}

\begin{abstract}
\noindent
According to Waag Society and the Research Group Crossmedia, recent studies have shown that
young adults are hard to reach when it comes down to (cultural) heritage.
Waag Society is researching how cultural heritage institutions can connect to these groups and 
how heritage objects can be relevant to (young) people.
Both parties believe that a better understanding of the emotions people have,
is very important to learn more about the way people value cultural heritage.
Therefore, Waag Society and the Research Group Crossmedia asked students of the HvA to
design an interactive tool that captures young adultsâ€™ emotions and enables them to discuss
these emotions with their peers when looking at (cultural) heritage.
\end{abstract}

\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Keywords}
Facial recognition, prototype, emotions, heritage, discussion, expressions

%------------------------------------------------

\section{Introduction}
During the minor 'Research in Emerging Technologies', we've chosen the project 'Emotions in Heritage' that
is commissioned by the Research Group Crossmedia (HvA) and the Waag Society. The Waag Society explores
emerging technologies not only related to the internet, but also related to biotechnology and cognitive sciences.
Art and culture often plays a central role in our research as well.

Our project involves systematically collecting the different emotions people experience when they perceive
(cultural) heritage, and use this information to spark meaningful conversations between different parties
by visualizing the different emotions. Cultural heritage could, for example, mean a painting, a building, or
even a tradition. Earlier research regarding emotion recognition has already been conducted. However, these
researches were mostly conducted from a psychological point of view; what emotions are being expressed
and why do young adults express a specific emotion? As such, models to define the emotions already exist.
Currently, there is a lack of instrumentation to capture these emotions and allow young adults to openly discuss
their emotions regarding heritage.

There are many options available to recognize emotions. For example, facial expression recognition, voice
recognition, text recognition, and wireless signals. Initially, we did research regarding these different
methods. We found a paper \cite{den2005facereader} which contained the various recognition methods and also included
the accuracy with which they were able to recognize emotions. Using these measurements, we made the choice to
limit how our project will recognize emotions to the method with the highest accuracy. The result of this
research concluded that facial expression recognition had the highest accuracy at an average of ~94.48\%
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth, scale=1]{emotion_recognition_accuracy.jpg}
    \caption{Accuracy of recognition methods}
\end{figure}

Besides accuracy, two more restraints are defined:
\begin{enumerate}
    \item{If at all possible, all code and libraries used must be open source.}
    \item{The prototype has to display the predicted emotions in an interactive way.}
\end{enumerate}
Due to these requirements, and our choice to use facial expression recognition, the main research question
of this paper is: \emph{Does the visualization of emotions of young people (aged 16-26) by facial expression
recognition software, lead to discussion regarding the displayed content?}

By working towards answering this question, we are developing a prototype that collects the relevant information
that is required for the goal of the project: starting a meaningful conversation between young adults, regarding
a specific subject. Furthermore, if the prototype proves to be useful and provides valid data, it might be used in subsequent
research as a tool for collecting data.

%------------------------------------------------

\section{Related work}
Our project, as defined by the Waag Society, is already based on two previous research projects. The first
project explores a method to sympathize with other people's emotions around heritage objects. Young people,
teachers, and heritage professionals each map their emotions around a certain heritage topic and use that
mapping as a starting point for discussion \cite{emotionnetworking2017}. The goal of this research is very
similar to our own, however the means to reach that goal differ, in that our research aims to use technology
to read the emotions of the participants, as opposed to mapping them themselves.

The second project examines how participation, narratives, digital media, and atmosphere affect museum 
visitors when visiting an exhibition and how exhibition makers can influence these visitor experiences. 
One of the research questions is how people are emotionally affected when encountering one of 
these means \cite{}. Just like the first project, our research differs due to our focus on the technological
aspect of detecting emotions.

Besides the above two project which were already defined at the start of the project, we've also found research
conducted towards facial expression recognition \cite{den2005facereader}. This research focuses solely on
detecting emotions using a camera. We also had the opportunity to try out the software created during this
research. Unlike this research, our research also seeks to visualize these predicted emotions in a meaningful
way in order to spark a discussion.

Lastly, we found a series of articles that aim to create open source facial expression recognition software
\cite{gent2016landmarks}. All of the chosen libraries and datasets were open source or easily available.
We have chosen these articles as the basis for our own prototype.

%------------------------------------------------

\section{Material}
Before creating the prototype, we were free in choosing \emph{how} we were going to implement the emotion
recognition. However, there were a couple restrictions and requirements. First, a very limited time span
to create the prototype in. The prototype had to be up and running in roughly 8 weeks so that we could
use it to perform our research. Secondly, preferable everything about the prototype had to be open source, as
requested by the Waag Society. Because of prior desk-research, combined with the above restrictions, we
chose to use facial expression recognition using a camera as the basis for our prototype. This decision was
further solidified upon the discovery of several articles regarding open source facial expression software
using Python, OpenCV, Dlib, and Facial Landmarks \cite{gent2016landmarks}.

Aside from the facial expression recognition part of the project, the prototype had to be able to visualize the
emotions to allow participants to reflect on their emotions and spark a discussion. The goal of the
visualization is to show the user of the prototype their emotions over a certain time span, and allow the user
to navigate through this time span. One of our clients, the Waag Society, had already done work regarding
this subject, including the classification of emotions \cite{plutchik1980general} which we will use, 
and a possible model for the visualization. We have used both the classification, as well as the model
as a basis for our prototype. The emotions we have used in our prototype are as follows: \emph{sadness, 
anger, contempt, disgust, fear, happiness, neutral, and surprise}.

In order to support our user research, the prototype has to go through a number of steps. Below, each of these
steps is explained in more detail:
\begin{enumerate}
    \item{Show a video to the research subject, and start filming at the same time. Store the video for analysis.}
    \item{Once the video is done, analyze the video and store the results of the analysis.}
    \item{Visualize the generated model on-screen when the user is ready.}
\end{enumerate}

The first step, recording and storing the video, is done with OpenCV\footnote{\url{https://opencv.org}} 
to allow video capture from the webcam. The prototype records at 10 frames per second (FPS).
Next, the video captures are stored locally on disk.
Displaying the video that the user of the prototype watches while he is being recorded, is considered part of
the visualization, and will be discussed later in this chapter.

The analysis and prediction of emotion from video frames is largely the same as described in the series of
articles written by Paul van Gent \cite{gent2016landmarks}. We use a trained, linear SVM machine learning model
to predict the emotions on someone's face, based on 68 specific facial landmarks. The linear SVM machine learning
model was created with scikit-learn\footnote{\url{https://scikit-learn.org}}, an open source machine learning library
for Python. In order to train the SVM model, we needed a suitable dataset of images, labeled with the
accompanying emotions. For this, we used the Cohn-Kanade and Extended Cohn-Kanade 
\cite{kanade2000comprehensive, lucey2010extended} data sets.
Each image in the dataset is preprocessed before being used to train the SVM model. This includes detecting
a face on each image, cropping and resizing the face to a common size, removing all color, and using OpenCV's
CLAHE algorithm. Once the preprocessing and sorting of the images is done, we use Dlib's
\footnote{\url{https://dlib.net}} shape predictor to fetch the 68 specific facial landmarks. These landmarks,
together with the label describing the emotion on the face, are then used to train the SVM model. The total
dataset used to train the model contains 834 images. Using a larger dataset will likely result in a more accurate,
stable, result.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth, scale=1]{landmarks.png}
    \caption{68 facial landmarks}
\end{figure}

The recorded video from our prototype is analyzed frame by frame. Each frame goes through the same preprocessing
steps as the dataset used to train the SVM model. Once preprocessed, the 68 landmarks from each frame are then
sent through the SVM model. Since this is a probabilistic model, the percentage chance for each of the 8 emotions
is returned as a result. We found that these results could fluctuate fairly rapidly in a short number of frames.
In order to get a more stable result, we've used the average of 5 each emotion over a span of 5 frames (half a
second). This means that our final prototype predicts the emotions for every half a second of recorded video.
Once the predictions are made for the entire video, the results are stored in an Sqlite database to be used
later for the visualization. This also allows us to store the results for analysis later in our research.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth, scale=1]{visualization.png}
    \caption{Visualized model as generated by the prototype}
\end{figure}

The visualization itself is a web application. A Flask\footnote{\url{http://flask.pocoo.org}} webserver 
is used to allow API access to the rest of the prototype. HTML, CSS and Javascript are then used to 
create the user interface for the prototype, including the visualization of the model.
Initially, we wanted to use Qt\footnote{\url{https://qt.io}}, a C++ UI library, but due to a lack of time and
expertise on our side, we decided to opt for something more familiar, which in this case resulted in a web
application. The user interface consists of four pages. One to display the video (and start the recording),
one to manage the sessions with our research participants, one to create a new session, and lastly, one to
show the visualization. The visualization page uses doughnut charts for each recorded participants. The charts
are generated using Chart.js\footnote{\url{http://chartjs.org}}, a Javascript library for creating a multitude
of charts. The predicted emotions are displayed half a second at a time, with a time line slider to allow the
participant to scroll through the predictions throughout the video.

%------------------------------------------------

\section{Method} \label{method}
After thoroughly testing the prototype on every aspect, working towards answering the main question was imminent.
\emph{Does the visualization of emotions of young people (aged 16-26) by facial expression
recognition software, lead to discussion regarding the displayed content?}

The research that we performed was qualitative research. Our prototype was built for testing purposes, and not
as an end-product. Therefore, we greatly value any feedback and improvement suggestions for our prototype from
our research participants, as this can be used in future development. Because we are researching
different emotional reactions of users, we're looking for very specific results. This means that our research
design is very specific as well. The research design has the following requirements:
\begin{itemize}
    \item{Users of the prototype have to be in a closed-off room to prevent any distractions which can affect
    test results.}
    \item{Users of the prototype will not be given any information regarding the purpose or operation of the
    prototype, since this can affect their emotional reactions and therefore the test results.}
    \item{Users will be asked to use the prototype one-by-one.}
    \item{Users of the prototype will be asked a set of questions which will be focused on accuracy
    and improvement of the prototype.}
    \item{Each video shown by the prototype has a maximum length of one minute due to technical restrictions.}
\end{itemize}

\subsection{Participants}
The participants are aged between 16-26 years old, and were scouted at the Amsterdam University of Applied
Sciences. The participants were invited into a reserved, closed-off classroom, located at the Wibautstraat, where
they were able to test the prototype. Our goal was to invite at least 20 participants. Unfortunately, due to
technical difficulties and time restrictions, we've only managed to get 13 participants.

\subsection{Content}
The video had to provoke a measurable reaction from the subjects. Therefore, it is important that we show content
that is doing just that. Research conducted by the Journal of Social Psychology \cite{leary2015seemingly}
has shown that the strongest emotional reaction by people is caused by showing pictures of traumatic events.
However, we wanted our subjects to show multiple emotions, and not just the ones that are associated with
traumatic events, like \emph{fear} and \emph{disgust}. Therefore, we decided to show the subjects content
that is, in any way, provoking. We have selected three videos which have been edited to last roughly one minute:
\begin{enumerate}
    \item{Roast of Giel Beelen (Peter Pannekoek roast)}
    \item{Asian longnecks: Why does this culture consider this beauty?}
    \item{Failarmy, best fails of 2017}
\end{enumerate}

%------------------------------------------------

\section{Results}
As described in \ref{method}, we have conducted prototype testing sessions with 13 participants. For each of these
participants, we have two pieces of data:
\begin{itemize}
    \item{Predicted emotions during the shown video, created by the prototype}
    \item{Results from the interview we had with each participant during the testing}
\end{itemize}
Furthermore, for every of the three available videos, we have expectations regarding the emotions at certain
times of the video. Using these pieces of data, we are interested in the following information from analyzing
the results:
\begin{itemize}
    \item{Do the participants agree with the emotions predicted by the prototype?}
    \item{Do the predicted emotions coincide with the expectations we had from certain videos?}
\end{itemize}

%------------------------------------------------

\section{Discussion}
To be added

%------------------------------------------------

\section{Conclusion}
To be added

%------------------------------------------------

\section{Acknowledgment}
This research was supported by the Amsterdam University of Applied Sciences, the Waag Society research
institute, and Research Group Crossmedia. We thank Bernadette Schrandt for all the assistance
with the research, including guidance and feedback. We thank Lodewijk Loos and Douwe-Sjoerd Boschman 
from Waag Society for the assistance during the creation of the prototype. We thank Wouter Meys for setting
up the minor and advice with the research. Lastly, we would like to thank all the participants who helped
us test our final prototype.

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}

%----------------------------------------------------------------------------------------

\end{document}
